TOPICS:

    • Hadoop ecosystem and components 
    • Hadoop architecture
    • HDFS :   Basic Concepts (HDFS, HDFSUseCases, BlockStructure)
    • understand difference phases in a Map reduce execution (Map,Sort,Shuffle and reduce, read recorder , mapper , reducer) .
    • Type of mappers and reducers
    • Installation of Hadoop on box, successfully starting all above entities.
    • Installations : Hadoop and jdk , Hadoop cluster setup 
    • Bashrc  , bash_profile  configuration for Hadoop and java 
    • VNC and Influxdb installation 
    • Kerberos setup
    • Daemons,functioning and communication between them(Namenode,Datanode,Secondary)
    • XML files (core-site.xml,hdfs-site.xml,mapred-site.xml)
    • Configfiles and parameters
    • Configuring Hadoop daemons 
    • Web HDFS rest API : File and directory operations including create , read, open , rename , delete
    • HDFS Commands : fsck , ls, mkdir , touchz, du ,cat, text , put , get , reading and writing commands in hdfs 
    • HDFS Replication : Distribution of files splits/Replication
    • Understanding HDFS web interface : NN , RESOURCE MANAGER , HBASE , Job Tracker/ Task Tracker 
    • Blocks and significance of block size.
    • Hadoop encryption and security management 
    • configured HIVE and SPARK
    • KeyValue type Programming, basic flow of data/logic in MapReduce
    • Hadoop high availability : failover , zookeeper , zkfc
    • Job Scheduler Basics : fifo , fair , capacity
    • Word count job , wordcount program , and it’s execution
    • Data locality
    • Partitioner , Comparators and Combiner
    • Sonarqube with Jenkins 
    • Ambari cluster setup
------------------------------------------------------------------------------------------------------------------------

Hadoop ecosystem and components

Hadoop Distributed File System (HDFS):

Hadoop 
Hadoop is a framework for working with big data
ramework that was created to make it easier to work with big data. It provides a method to access data that is distributed among multiple clustered computers, process the data, and manage resources across the computing and network resources that are involved. process large data sets that reside in clusters of computers.
4 core modules : hdfs , yarn , maoreduce , common
    • Hdfs : Provides access to application data. 
    • Yarn : schedule jobs and manage resources across the cluster that holds the data
    • Map reduce : YARN-based parallel processing system for large data sets.
    • Common : set of utilities that supports the three other core modules.Ecosystem 
it has projects to extend Hadoop capabilities to make it easier to use.
includes both official Apache open source projects and a wide range of commercial tools and solutions. 
    • open source examples include Spark, Hive (hcatalog , webhcat , hiveql), Pig, Oozie , hbase , and Sqoop. ++ ambari , avro , Cassandra , chukwa , impala , flume , kafka , mahout , tajo , tez, zookeeper.
    • Commercial : Cloudera, Hortonworks, and MapR, 

1. Spark : 
for data processing , independently of Hadoop
    • gateway to in-memory computing for Hadoop, alternative to MapReduce that enables workloads to execute in memory, instead of on disk. Spark accesses data from HDFS but bypasses the MapReduce processing framework.
    • Support sql , works with scala , python and r shell .
    • Use : data extract/transform/load (ETL) operations, stream processing, machine learning development and with the Apache GraphX API for graph computation and display. Spark can run on a variety of Hadoop and non-Hadoop clusters, including Amazon S3.
    2. Hive : 
The Hadoop ecosystem component, Apache Hive, is an open source data warehouse system for querying and analyzing large datasets stored in Hadoop files. Hive do three main functions: data summarization, query, and analysis.
Hive use language called HiveQL (HQL), which is similar to SQL. HiveQL automatically translates SQL-like queries into MapReduce jobs which will execute on Hadoopfor data structrng , and querying , write queries , provide etl operatin , sql capability , devlop app for Hadoop env.
    • Components : hcatalog , webhcat , hiveql
    • Hcatalog:  Helps data processing tools read and write data on the grid. It supports MapReduce and Pig
    • Webhcat:  Lets you use an HTTP/REST interface to run MapReduce, Yarn, Pig, and Hive jobs.
    • Hiveql: hive query lang , same as sql , structure and query data 

    3. Pig:
Procedural lang for developing parallel processing app for  large data sets , alternative to java in mapreduce , automatically genrte mapreduce fun ,.
    • Includes pig latin (scripting lang0) p, pig translates pig latin scripts into mapr reduce , automate mapreduce complexity
    • Used : mul data operation , devlp app tht sorts data , write their own functions .
    4. Hbase :
    • scalable, distributed, NoSQL database that sits atop the HFDS. store structured data in tables , HBase is not a relational database and wasn’t designed to support transactional and other real-time applications. 
    • accessible through a Java API and has ODBC and JDBC drivers. ,  support SQL queries , Hive can be used to run SQL-like queries in HBase
    5. oozie
It is a workflow scheduler system for managing apache Hadoop jobs. Oozie combines multiple jobs sequentially into one logical unit of work. Oozie framework is fully integrated with apache Hadoop stack, YARN as an architecture center and supports Hadoop jobs for apache MapReduce, Pig, Hive, and Sqoop.
HBase
Apache HBase is a distributed database that was designed to store structured data in tables that could have billions of row and millions of columns. HBase is scalable, distributed, and NoSQL database that is built on top of HDFS. HBase, provide real-time access to read or write data in HDFS.
    • workflow scheduler , manage workflow 
    • server-based Java web application that uses workflow definitions written in hPDL, which is an XML Process Definition 
    • supports specific workflow types, so other workload schedulers are commonly used
    6. sqoop:
    • command-line interface that facilitates moving bulk data from Hadoop into relational databases and other structured data stores. 
    • No script needed to process data , move from warehouse to Hadoop cluster 
    7. Ambari 
–ool 4 managing, and monitoring Apache Hadoop clusters which includes support for Hadoop HDFS, Hadoop MapReduce, Hive, HCatalog, HBase, ZooKeeper, Oozie, Pig, and Sqoop.
    8. Avro – A data serialization system.
    9. Cassandra – A scalable multi-master database with no single points of failure.
    10. Chukwa – A data collection system for managing large distributed systems.
    11. Impala – The open source, native analytic database for Apache Hadoop. Impala is shipped by Cloudera, MapR, Oracle, and Amazon.
    12. Flume – A distributed, reliable, and available service for efficiently collecting, aggregating, and moving large amounts of streaming event data.
    13. Kafka – A messaging broker that is often used in place of traditional brokers in the Hadoop environment because it is designed for higher throughput and provides replication and greater fault tolerance.
    14. Mahout – A scalable machine learning and data mining library.
    15. Tajo –
    • big data relational and distributed data warehouse system. 
    • designed for low-latency and scalable ad-hoc queries, online aggregation, and ETL on large-data sets stored on HDFS and other data sources. 
    • By supporting SQL standards and leveraging advanced database techniques, Tajo allows direct control of distributed execution and data flow across a variety of query evaluation strategies and optimization opportunities.
    16. Tez –data-flow programming framework, built on Hadoop YARN, which provides a powerful and flexible engine to execute an arbitrary DAG of tasks to process data for both batch and interactive use-cases. Tez is being adopted by Hive, Pig and other frameworks in the Hadoop ecosystem, commercial software (e.g. ETL tools), to replace MapReduce as the underlying execution engine.
    17. Zookeper – A high-performance coordination service for distributed applications.a centralized service component for maintaining configuration information, naming, providing distributed synchronization, and providing group services. Zookeeper manages and coordinates a large cluster of machines.

 HDFS is the primary storage system of Hadoop. Hadoop distributed file system (HDFS) is a java based file system that provides scalable, fault tolerance, reliable and cost efficient data storage for Big data. 
HDFS  Components: NameNode and DataNode.
 NameNode
Master node. NameNode does not store actual data or dataset. NameNode stores Metadata i.e. number of blocks, their location, on which Rack, which Datanode the data is stored and other details. It consists of files and directories.
Tasks :
Manage file system namespace ,  Regulates client’s access to files , Executes file system execution such as naming, closing, opening files and directories
DataNode
Slave. HDFS Datanode is responsible for storing actual data in HDFS , performs read and write operation as per the request of the clients. Replica block of Datanode consists of 2 files on the file system. The first file is for data and second file is for recording the block’s metadata. HDFS Metadata includes checksums for data. At startup, each Datanode connects to its corresponding Namenode and does handshaking. Verification of namespace ID and software version of DataNode take place by handshaking. At the time of mismatch found, DataNode goes down automatically.
MapReduce 
provides data processing. MapReduce is a software framework for easily writing applications that process the vast amount of structured and unstructured data stored in the Hadoop Distributed File system.
MapReduce programs are parallel in nature, Thus, it improves the speed and reliability of cluster this parallel processing.
Hadoop Ecosystem component ‘MapReduce’ works by breaking the processing into two phases:
Map phase
Reduce phase
Each phase has key-value pairs as input and output. In addition, programmer also specifies two functions: map function and reduce function
Map function takes a set of data and converts it into another set of data, where individual elements are broken down into tuples (key/value pairs). 
Reduce function takes the output from the Map as an input and combines those data tuples based on the key and accordingly modifies the value of the key. Read Reducer in detail.
YARN
Hadoop YARN (Yet Another Resource Negotiator) , provides the resource management. It is operating    system of Hadoop as it is responsible for managing and monitoring workloads. It allows multiple data processing engines such as real-time streaming and batch processing to handle data stored on a single platform.


------------------------------------------------------------------------------------------------------------------------
Hadoop architecture
Hadoop has a master-slave topology. In this topology, we have one master node and multiple slave nodes. Master node’s function is to assign a task to various slave nodes and manage resources. The slave nodes do the actual computing. Slave nodes store the real data whereas on master we have metadata. This means it stores data about data. 
Hadoop Architecture comprises three major layers:
HDFS (Hadoop Distributed File System)
Yarn :
MapReduce 
Hdfs : Node Manager offers some resources to the cluster. Its resource capacity is the amount of memory 
NameNode
single master server , single point failure.
It manages the file system namespace by executing an operation like the opening, renaming and closing the files.
DataNode
The HDFS cluster contains multiple DataNodes. Each DataNode contains multiple data blocks.
These data blocks are used to store data. responsibility is to read and write requests from the file system's clients. It performs block creation, deletion, and replication upon instruction from the NameNode
Yarn : providing the computational resources (e.g., CPUs, memory, etc.) needed for application executions. Two important elements are: node and resource manager 
Resource manager : knows where the slaves are located (Rack Awareness) and how many resources they have , how to assign the resources. To manage the use of resources across the cluster
Node manager : sends an heartbeat to the Resource Manager , Node Manager offers some resources to the cluster. Its resource capacity is the amount of memory To manage the use of resources across the cluster
Mapreduce : data processing tool which is used to process the data parallelly in a distributed form
two phases, the mapper phase, and the reducer phase.
Process : map takes data in the form of pairs and returns a list of <key, value> pairs. 
sort and shuffle are applied , This sort and shuffle acts on these list of <key, value> pairs and sends out unique keys and a list of values associated with this unique key <key, list(values)>.
output of sort and shuffle sent to the reducer phase. The reducer performs a defined function on a list of values for unique keys, and Final output <key, value> will be stored/displayed.














The default size of the HDFS block is 128MB , it is not necessary that in HDFS, each file stored should be an exact multiple of the configured block size 128mb, 256mb etc., so final block for file uses only as much space as is needed.
Simplicity of storage management
Ability to store very large files
Fault tolerance and High Availability of HDFS
Simple Storage mechanism for datanodes
Map reduce execution (Map,Sort,Shuffle and reduce, read recorder) .
input reader : The input reader reads the upcoming data and splits it into the data blocks of the appropriate size (64 MB to 128 MB). Each data block is associated with a Map function.Once input reads the data, it generates the corresponding key-value pairs. The input files reside in HDFS.
Map function :The map function process the upcoming key-value pairs and generated the corresponding output key-value pairs. The map input and output type may be different from each other.
Partition function: The partition function assigns the output of each Map function to the appropriate reducer. The available key and value provide this function. It returns the index of reducers.
Shuffling and Sorting :The data are shuffled between/within nodes so that it moves out from the map and get ready to process for reduce function. Sometimes, the shuffling of data can take much computation time.
The sorting operation is performed on input data for Reduce function. Here, the data is compared using comparison function and arranged in a sorted form.
Reduce function
The Reduce function is assigned to each unique key. These keys are already arranged in sorted order. The values associated with the keys can iterate the Reduce and generates the corresponding output.
Output writer
Once the data flow from all the above phases, Output writer executes. The role of Output writer is to write the Reduce output to the stable storage.
------------------------------------------------------------------------------------------------------------------------
Type of mappers and reducers

------------------------------------------------------------------------------------------------------------------------
VNC and Influxdb installation 


------------------------------------------------------------------------------------------------------------------------

Daemons,theirfunctioningandcommunicationbetweenthem(Namenode,Datanode,Secondary)


------------------------------------------------------------------------------------------------------------------------

Configuring Hadoop daemons 


------------------------------------------------------------------------------------------------------------------------

Web HDFS rest API : File and directory operations including create , read, open , rename , delete 


------------------------------------------------------------------------------------------------------------------------

HDFS Replication : Distribution of files splits/Replication


------------------------------------------------------------------------------------------------------------------------

Blocks and significance of block size.


------------------------------------------------------------------------------------------------------------------------

Hadoop encryption and security management 


------------------------------------------------------------------------------------------------------------------------

Hadoop high availability : failover , zookeeper , zkfc

------------------------------------------------------------------------------------------------------------------------
Job Scheduler Basics : fifo , fair , capacity


------------------------------------------------------------------------------------------------------------------------

Word count job , wordcount program , and it’s execution


------------------------------------------------------------------------------------------------------------------------

Data locality


------------------------------------------------------------------------------------------------------------------------

Partitioner , Comparators and Combiner

------------------------------------------------------------------------------------------------------------------------
Sonarqube with Jenkins


------------------------------------------------------------------------------------------------------------------------
installation : hadoop and java :
    • wget --no-cookies --no-check-certificate --header "Cookie: gpw_e24=http%3A%2F%2Fwww.oracle.com%2F; oraclelicense=accept-securebackup-cookie" "https://download.oracle.com/otn-pub/java/jdk/8u201-b09/42970487e3af4f5aa5bca3f542482c60/jdk-8u201-linux-x64.tar.gz"
    • tar xzf jdk-8u201-linux-x64.tar.gz
    • java -version
    • wget https://www-us.apache.org/dist/hadoop/common/hadoop-2.7.7/hadoop-2.7.7.tar.gz
    • which java
    • which hadoop
    • echo $JAVA_HOME 
    • echo $HADOOP_HOME
bashrc set  :
    • export JAVA_HOME=/root/jdk1.8.0_201/
    • export PATH=$JAVA_HOME/bin:$PATH
    • export CLASSPATH=.:$JAVA_HOME/jre/lib:$JAVA_HOME/lib:$JAVA_HOME/lib/tools.jar

    • export HADOOP_HOME=$HOME/hadoop-2.7.3
    • export HADOOP_PREFIX=$HOME/hadoop-2.7.3
    • export HADOOP_CONF_DIR=$HOME/hadoop-2.7.3/etc/hadoop
    • export HADOOP_MAPRED_HOME=$HOME/hadoop-2.7.3
    • export HADOOP_COMMON_HOME=$HOME/hadoop-2.7.3
    • export HADOOP_HDFS_HOME=$HOME/hadoop-2.7.3
    • export YARN_HOME=$HOME/hadoop-2.7.3
    • export HADOOP_OPTS="-Djava.library.path=$HOME/hadoop-2.7.3/lib/native"
    • export HADOOP_COMMON_LIB_NATIVE_DIR=$HOME/hadoop-2.7.3/lib/native
    • export PATH=$PATH:$HADOOP_HOME/sbin:$HOME/hadoop-2.7.3/bin
    • export PATH=$PATH:$HOME/hadoop-2.7.3/bin
    • export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
    • export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native"
    • export HADOOP_CLASSPATH=$HOME/hadoop-2.7.3/lib

EDIT CONFIGURATION FILE 
    • vi core-site.xml
    • vi hdfs-site.xml
    • vi mapred-site.xml.template
    • vi mapred-site.xml
    • vi yarn-site.xml
    • vi hadoop-env.sh
START SERVICES 
    • bin/hadoop namenode -format
    • start-dfs.sh
    • start-yarn.sh
    • jps
    • ss -tu;
    • ss -tul
    • ss -tuln
------------------------------------------------------------------------------------------------------------------------
Hadoop commands :

    •  make dir : hdfs dfs -mkdir /my_storage 
    •  prints the Hadoop version
    •  hdfs dfs version
    •  Creates any parent directories in path that are missing
    •  hdfs dfs -mkdir /user/dataflair/dir1
    •  displays a list of the contents of a directory 
    • hdfs dfs -ls /user/dataflair/dir1
    • copies the file or directory from the local file system to the destination within the DFS.
    • hdfs dfs -put /home/dataflair/Desktop/sample /user/dataflair/dir1
    • source is restricted to a local file reference.
    • hdfs dfs -copyFromLocal /home/dataflair/Desktop/sample /user/dataflair/dir1
    • copies the file or directory in HDFS identified by the source to the local file system path identified by local destination.
    • hdfs dfs -put /home/dataflair/Desktop/sample /user/dataflair/dir1
    • hdfs dfs -cat /user/dataflair/dir1/sample
    • hadoop fs -mv /user/dataflair/dir1/purchases.txt /user/dataflair/dir2
    • hadoop fs -cp /user/dataflair/dir2/purchases.txt /user/dataflair/dir1
    • hdfs dfs -tail -f /user/dataflair/dir2/purchases.txt
    • hdfs dfs -rm /user/dataflair/dir2/sample 
    • hdfs dfs -expunge  : empty the trash 
    • hdfs dfs -chown -R dataflair /opt/hadoop/logs 
    • hdfs dfs -chgrp [-R] New Group sample 
    • change the replication factor of a file : hdfs dfs -setrep -w 3 /user/dataflair/dir1 
    • hdfs dfs -du /user/dataflair/dir1/sample 
    • hdfs dfs -df -h
    • hdfs dfs -touchz /user/dataflair/dir2 
    • hdfs dfs -test -z sample : for file test operations.
    • takes a source file and outputs the file in text format. : hdfs dfs -text /user/dataflair/dir1/sample 
    • hdfs dfs -stat /user/dataflair/dir1 : prints path info
    • hdfs dfs -chmod 777 /user/dataflair/dir1/sample 
    • hdfs dfs -chown -R dataflair /opt/hadoop/logs
    • hadoop fs -appendToFile /home/dataflair/Desktop/sample /user/dataflair/dir1 
    • hdfs dfs -count /user/dataflair  :ounts the number of directories, number of files present and bytes under the paths
    • hadoop fs -find /user/dataflair/dir1/ -name sample -print 
    • put a file into created directry : hdfs dfs -put LICENSE.txt /my_storage
    • health check : hdfs fsck /
    • view content of a file , kept in a directry : hdfs dfs -cat  /my_storage/LICENSE.txt
    • help :  hdfs dfs -help
    • create empty file with size 0  : hadoop-2.7.3]# hdfs dfs -touchz /my_storage/anam
    • check file size :  hdfs dfs -du -s /my_storage/anam
    • hadoop fs -find /user/dataflair/dir1/ -name sample -print 
    • hadoop fs -truncate -w 127 /user/dataflair/dir2/purchases.txt : trim file to a given size 
      
------------------------------------------------------------------------------------------------------------------------
hive setup hdp cluster :
    • wget http://archive.apache.org/dist/hive/hive-2.1.0/apache-hive-2.1.0-bin.tar.gz
    • tar -xzf apache-hive-2.1.0-bin.tar.gz
    • ls
    • edit .bashrc 
    • export HIVE_HOME=/home/hadoop /apache-hive-2.1.0-bin
    • export PATH=$PATH:/home/hadoop /apache-hive-2.1.0-bin/bin
    • source .bashrc
    • hive –version
    •  hdfs dfs -mkdir -p /user/hive/warehouse
    •  hdfs dfs -mkdir /tmp
    •  hdfs dfs -chmod g+w /user/hive/warehouse
    •   hdfs dfs -chmod g+w /tmp
    •  set hive-env.sh : cd apache-hive-2.1.0-bin/ :conf/hive-env.sh
    • export HADOOP_HOME=/home/hadoop/Hadoop
    • export HADOOP_HEAPSIZE=512
    • export HIVE_CONF_DIR=/home/Hadoop/apache-hive-2.1.0-bin/conf
    • edit hive-site.xml
    • Launch hive : hive
    • Show databases;
    • Show tables;
    • Exit
    • 
<configuration>
<property>
<name>javax.jdo.option.ConnectionURL</name>
<value>jdbc:derby:;databaseName=/home/hadoop/apache-hive-2.1.0-bin/metastore_db;create=true</value>
<description>
JDBC connect string for a JDBC metastore.
To use SSL to encrypt/authenticate the connection, provide database-specific SSL flag in the connection URL.
For example, jdbc:postgresql://myhost/db?ssl=true for postgres database.
</description>
</property>
<property>
<name>hive.metastore.warehouse.dir</name>
<value>/user/hive/warehouse</value>
<description>location of default database for the warehouse</description>
</property>
<property>
<name>hive.metastore.uris</name>
<value/>
<description>Thrift URI for the remote metastore. Used by metastore client to connect to remote metastore.</description>
</property>
<property>
<name>javax.jdo.option.ConnectionDriverName</name>
<value>org.apache.derby.jdbc.EmbeddedDriver</value>
<description>Driver class name for a JDBC metastore</description>
</property>
<property>
<name>javax.jdo.PersistenceManagerFactoryClass</name>
<value>org.datanucleus.api.jdo.JDOPersistenceManagerFactory</value>
<description>class implementing the jdo persistence</description>
</property>
</configuration>
------------------------------------------------------------------------------------------------------------------------
spark setup hdp cluster :
    • Wget https://www-eu.apache.org/dist/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz
    • tar -xvf spark-2.3.3-bin-hadoop2.7.tgz
    • cd  spark-2.4.0-bin-hadoop2.7/bin
    • ./spark-shell
    • Set bashrc
    • SPARK_HOME=/home  /local/spark
    • export PATH=$SPARK_HOME/bin:$PATH
    • source ~/.bashrc
------------------------------------------------------------------------------------------------------------------------
Hadoop cluster setup steps:
1) Install JDK 
    • add Oracle's PPA
sudo add-apt-repository ppa:webupd8team/java
    • update your package repository
sudo apt-get update
    • install jdk 8
 sudo apt-get install oracle-java8-installer
2) Open ssh setup
    • enable ssh in ubuntu :
sudo apt-get install openssh-server
    • SSH service enabled
sudo service ssh status
    • change settings by editing configuration file 
sudo nano /etc/ssh/sshd_config
    • restart ssh
sudo service ssh restart
3) Hadoop apache cluster installation 
    • check java version 
java -version
    • create hadoop account with username and password
adduser hadoop
passwd hadoop
    • setup keybased ssh into account
su -hadoop
ssh-keygen -t rsa
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
chmod 0600 ~/.ssh/authorized_keys
    • verify keybased login and exit 
ssh localhost
exit 
    • download hadoop 2.7.7
wget http://www-eu.apache.org/dist/hadoop/common/hadoop-2.7.7/hadoop-2.7.7.tar.gz
tar xzf hadoop-2.7.7.tar.gz 
    • rename the folder 
mv hadoop-2.6.5 hadoop
4 ) Setup environment variables
    • edit ~/.bashrc file
export HADOOP_HOME=/home/hadoop/hadoop
export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
    • apply changes 
source ~/.bashrc
    • edit HADOOP_HOME/etc/hadoop/hadoop-env.sh
and edit JAVA_HOME variables 
export JAVA_HOME=/opt/jdk1.8.0_131/
5) Edit configuration file 
    • cd $HADOOP_HOME/etc/Hadoop
    • core-site.xml
 <configuration>
<property>
<name>fs.default.name</name>
<value>hdfs://localhost:9000</value>
</property>
</configuration>
    • hdfs-site.xml
<configuration>
<property>
<name>dfs.replication</name>
<value>1</value>
</property>
<property>
<name>dfs.name.dir</name>
<value>file:///home/hadoop/hadoopdata/hdfs/namenode</value>
</property>
<property>
<name>dfs.data.dir</name>
<value>file:///home/hadoop/hadoopdata/hdfs/datanode</value>
</property>
</configuration>
    • mapred-site.xml
<configuration>
 <property>
<name>mapreduce.framework.name</name>
<value>yarn</value>
</property>
</configuration>
    • yarn-site.xml
<configuration>
 <property>
<name>yarn.nodemanager.aux-services</name>
<value>mapreduce_shuffle</value>
</property>
</configuration>
6) Format namenode 
    •  hdfs namenode -format
7) Start hadoop cluster 
    • cd $HADOOP_HOME/sbin/
8) Run the scripts -
    • start-dfs.sh
    • start-yarn.sh

9) Access hadoop services in the browser 
    • <ip:50070>
    • to get information on clusters :
access port 8088 : <ip:8088>
    • check secondary namenode:
by port  50090 <ip:50090>
    • check datanode
by port 50075 <IP:50075>
10 ) Test hadoop single node setup 
    • make HDFS directories
bin/hdfs dfs -mkdir /user
bin/hdfs dfs -mkdir /user/hadoop
    • copy all the files from local /var/log/https to HDFS
bin/hdfs dfs -put /var/log/httpd logs
    • browse HDFS in browser
http://svr1.tecadmin.net:50070/explorer.html#/user/hadoop/logs
    • copy logs directory for HDFS to local file system 
bin/hdfs dfs -get logs /tmp/logs
ls -l /tmp/logs/

hdp repos :
http://public-repo-1.hortonworks.com/HDP/centos7/3.x/updates/3.0.1.0
http://public-repo-1.hortonworks.com/HDP-GPL/centos7/3.x/updates/3.0.1.0
------------------------------------------------------------------------------------------------------------------------
Define a rack topology script
etc/hadoop/conf
rack-topology.sh

#!/bin/bash
# Adjust/Add the property "net.topology.script.file.name"
# to core-site.xml with the "absolute" path the this
# file. ENSURE the file is "executable".
# Supply appropriate rack prefix
RACK_PREFIX=default
# To test, supply a hostname as script input:
if [ $# -gt 0 ]; then
CTL_FILE=${CTL_FILE:-"rack_topology.data"}
HADOOP_CONF=${HADOOP_CONF:-"/etc/hadoop/conf"} 
if [ ! -f ${HADOOP_CONF}/${CTL_FILE} ]; then
 echo -n "/$RACK_PREFIX/rack "
 exit 0
fi
while [ $# -gt 0 ] ; do
 nodeArg=$1
 exec< ${HADOOP_CONF}/${CTL_FILE}
 result=""
 while read line ; do
 ar=( $line )
 if [ "${ar[0]}" = "$nodeArg" ] ; then
 result="${ar[1]}"
 fi
 done
 shift
 if [ -z "$result" ] ; then
 echo -n "/$RACK_PREFIX/rack "
 else
 echo -n "/$RACK_PREFIX/rack_$result "
 fi
done
else
 echo -n "/$RACK_PREFIX/rack "
fi

rack_topology.data
# This file should be:
# - Placed in the /etc/hadoop/conf directory
# - On the Namenode (and backups IE: HA, Failover, etc)
# - On the Job Tracker OR Resource Manager (and any Failover JT's/RM's) 
# This file should be placed in the /etc/hadoop/conf directory.

# Add Hostnames to this file. Format <host ip> <rack_location>
192.168.2.12 03

Run the topology script to ensure that it returns the correct rack information for each host.

stop hdfs cluster

core-site.xml.
<property>
  <name>net.topology.script.file.name</name> 
  <value>/etc/hadoop/conf/rack-topology.sh</value>
</property>
<property> 
  <name>net.topology.script.number.args</name> 
  <value>75</value>
</property>

restart hdfs and mapreduce 
hdfs fsck
chk /var/log/hadoop/hdfs/ -> new node /rack01/ip
dfsadmin -report

managing locall users in hdp : UI 
USERS>ADD >DEL
SAME WITH USERS 

Decommissioning Slave Nodes
hdfs-site.xml

<property> 
 <name>dfs.hosts.exclude</name> 
 <value><HADOOP_CONF_DIR>/dfs.exclude</value>
 <final>true</final> 
 </property> 

yarn-site.xml
<property> 
 <name>yarn.resourcemanager.nodes.exclude-path</name> 
 <value><HADOOP_CONF_DIR>/yarn.exclude</value>
 <final>true</final> 
</property>

both a DataNode and a NodeManager, and both are typically commissioned or decommissioned together.

DATANODE 
<HADOOP_CONF_DIR>/dfs.exclude 
su <HDFS_USER> 
hdfs dfsadmin -refreshNodes

http://<NameNode_FQDN>:50070 :nn ui
------------------------------------------------------------------------------------------------------------------------
Ambari cluster script 
Repos , selinux , firewall, bashrc , hosts , postgress , , setup , oozie and hive table , ambari server agent install n strt 

    #!/bin/bash
    #check for repos
    cd /etc/yum.repos.d
     rm -rf ambari.repo hdp.repo hdp-utils.repo hdp-gpl.repo
     echo "set the values for the repos removed, the values can be taken from there corresponding      files"
     cat /root/ambari/ambari.txt > ambari.repo
    cat /root/ambari/hdp.txt > hdp.repo
    cat /root/ambari/hdp-utils.txt > hdp-utils.repo
     cat /root/ambari/hdp.txt > hdp-gpl.repo
     #selinux disable
    cd
    sestatus
     echo 0 > /selinux/enforce
     #disable firewall
     sudo systemctl stop firewalld
     sudo systemctl disable firewalld
     #check java
     cd
     echo $JAVA_HOME
     echo $HADOOP_HOME
     echo "add java path to bashrc and source it"
     cat /root/ambari/bashrc.txt > .bashrc
     #for heartbeat
     echo " manually enter ip and fqdn in /etc/hosts "
     #install postgress and configure
     yum install postgresql-jdbc
     ambari-server setup –-jdbc-db=postgres –-jdbc-driver=/usr/share/java/postgresql-jdbc.jar
     echo "set the ip and the host at the end of the file for oozie and hive "
     su - postgres
     cd data
     cat /root/ambari/hosts.txt > pg_hba.conf
     CREATE DATABASE oozie;
     CREATE USER oozie with PASSWORD 'bigdata';
     GRANT ALL PRIVILEGES ON DATABASE oozie to oozie;
     CREATE DATABASE hive;
     CREATE USER hive with PASSWORD 'bigdata';
     GRANT ALL PRIVILEGES ON DATABASE hive to hive;
     echo "hive and oozie configured"
     #ambari install and start
     echo "installing ambari agent and ambari server"
     yum install ambari-server
     ambari-server start
     ambari-server status
     ambari-server setup
     echo "amabari server started "
     yum install ambari-agent
     ambari-agent start
     ambari-agent status
     echo "ambari agent started "
 
ambari curl api :
impadmin@cdhclusterserver:~$ curl -u admin:admin -H "X-Requested-By: ambari" -X GET http://172.26.72.236:8080/api/v1/clusters/Ambari/services
curl --user admin:admin http://172.26.72.236:8080/api/v1/clusters
curl -u admin:admin -H "X-Requested-By: ambari" -X GET http://172.26.72.236:8080/api/v1/clusters/Ambari/components
------------------------------------------------------------------------------------------------------------------------
Kerberos setup
Steps:
    1. Install Kerberos
    2. Using Kerberos 
    3. key distribution center (KDC) for the Hadoop cluster
    4. create service principals for each of the Hadoop services for example mapreduce, yarn and    hdfs.
    5. create encrypted kerberos keys (keytabs) for each service principal
    6. distribute keytabs for service principals to each of the cluster nodes.
    7. configuring all services to rely on kerberos authentication.

Step: 1: Install Kerberos
    • From impadmin/Hadoop account run : Sudo apt-get install krb5-user
    • Enter  local password for the account 
    • Set Kerberos 5 realm like .com, .uk ..  with a password
Step :2: Using Kerberos 
    • Kinit <user>
    • Enter password
    • Show all tickets : klist
Step :3: key distribution center (KDC) for the Hadoop cluster
    • run some Hadoop commands :
hadoop fs -mkdir /usr/Hadoop
    • install the client. The client will be used to request for tickets from KDC.
sudo apt install krb5-user libpam-krb5 libpam-ccreds auth-client-config
    • To install the server and KDC
sudo apt-get install krb5-kdc krb5-admin-server
    • initialize a new realm on the machine that will act as KDC server &Enter a password 
sudo krb5_newrealm
    • edit kdc.conf file 
sudo gedit /etc/krb5kdc/kdc.conf
change : data files will be located, period tickets remain valid and the realm name(ex:EDUONIX.COM.)
    • creating a KDC database 
kdb5_util create -r EDUONIX.COM -s  to create the database and use a master key of your choice.
    • edit acl file to include the kerberos principal of administrators. 
sudo gedit /etc/krb5kdc/kadm5.acl 
add this line */admin@EDUONIX.COM *
    • kadmin.local utility is used to add users who are administrators to the kerberos database
sudo kadmin.local
add user : addprinc eduonix/admin@EDUONIX.COM
    • Start kerberos services
service krb5kdc start
service kadmin start
    • to reconfigure kerberos 
sudo dpkg-reconfigure krb5-kdc.
    • Edit  kerberos client configuration settings 
sudo gedit /etc/krb5.conf
EDUONIX.COM = {
kdc = kerberos.eduonix.com
admin_server = kerberos.eduonix.com
}
    • domain_realm:
.eduonix.com = EDUONIX.COM
eduonix.com = EDUONIX.COM
    • to edit SSH configuration to allow kerberos authentication to be used.
sudo gedit /etc/ssh/sshd_config
# Kerberos options
KerberosAuthentication yes
KerberosGetAFSToken no
KerberosOrLocalPasswd yes
KerberosTicketCleanup yes 
    • edit hadoop core-site.xml
sudo gedit /usr/local/hadoop/hadoop-2.7.1/etc/hadoop/core-site.xml
<property>
<name>hadoop.security.authentication</name>
<value>kerberos</value> <!-- A value of "simple" would disable security. -->
</property>

<property>
<name>hadoop.security.authorization</name>
<value>true</value>
</property>
    • Mkdir : hadoop fs -mkdir /usr/local/kerberos2

Step:4:  create service principals for each of the Hadoop services for example mapreduce, yarn and hdfs.
    • using kerberos to authenticate users and services.
create as many principals as there are users, now create user group for every user account , then create principals 
    • create user accounts for hdfs, mapred, yarn and learner:
sudo adduser hdfs
sudo adduser hdfs hadoop
sudo adduser mapred
sudo adduser mapred Hadoop
sudo adduser yarn
sudo adduser yarn hadoop
sudo adduser learner
sudo adduser learner Hadoop
    • create principals:
sudo kadmin.local
addprinc learner@LOCALHOST
             addprinc hdfs@LOCALHOST
addprinc mapred@LOCALHOST
addprinc yarn@LOCALHOST
listprincs

Step:5: create encrypted kerberos keys (keytabs) for each service principal
    • kinit command to request tickets
encrypted Kerberos keys -> keytabs (for passwordless communication and authentication)
    • keytab creation :for http and hdfs 
sudo kadmin.local
xst -norandkey -k hdfs.keytab hdfs@LOCALHOST HTTP@LOCALHOST
or 
ktadd -k dse.keytab dse/FQDN -> ktadd -k /tmp/node1.keytab dse/node1.example.com
    • keytab creation :for mapred
xst -norandkey -k mapred.keytab mapred@LOCALHOST HTTP@LOCALHOST
    • keytab creation :for yarn
xst -norandkey -k mapred.keytab yarn@LOCALHOST HTTP@LOCALHOST
    • inspect keytabs :
klist -e -k -t hdfs.keytab
    • deploy keytabs : moving them to a directory under etc directory.
    • sudo mv hdfs.keytab mapred.keytab /etc/hadoop/conf/
    • sudo mv hdfs.keytab mapred.keytab yarn.keytab /etc/hadoop/conf/
    • check /etc/hadoop/conf/
    • To make them readable only by their respective users by assigning their ownership to correct users.
    I. sudo chown hdfs:hadoop /etc/hadoop/conf/hdfs.keytab
    II. sudo chown mapred:hadoop /etc/hadoop/conf/mapred.keytab
    III. sudo chmod 400 /etc/hadoop/conf/*.keytab
    • To authenticate via kerberos with human interaction you use the kinit command to request tickets. You need to specify the keytab and the principal requesting an access ticket.
    • kinit -k -t hdfs.keytab hdfs
------------------------------------------------------------------------------------------------------------------------
COMMON ISSUE :

WARNING: HADOOP_PREFIX has been replaced by HADOOP_HOME. Using value of HADOOP_PREFIX.
19/04/24 12:01:59 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
yarn dns registry failure 
HADOOP_PREFIX has been replaced by HADOOP_HOME. Using value of HADOOP_PREFIX.
-----------------------------------------------------------------------------------------------------------
datanode not cooming up :
<name>dfs.namenode.name.dir</name>
      <value>/hadoop/hdfs/namenode</value>
I
clusterID=CID-9b85109d-1af7-41d1-a21e-34d7d3cb54b2
cTime=1556096269649
storageType=NAME_NODE
blockpoolID=BP-795239126-172.26.41.60-1556096269649
layoutVersion=-64
-----------------------------------------------------------------------------------------------------------
blockpool not registered :

  <name>dfs.datanode.data.dir</name>
      <value>/hadoop/hdfs/data</value>
namespaceID=297559242
storageID=DS-26fbd52b-7fe8-4673-a119-3b740ef6c417
clusterID=CID-9c552c7a-5e49-4798-b2ed-e448875c1ef7
cTime=0
datanodeUuid=e9fbf1ef-d7d5-45c6-bec3-c949f3e81474
storageType=DATA_NODE
layoutVersion=-57
--------------------------------------------------------------------------------
no pg_hba.conf entry for host "[local]", user "root", database "root", SSL off
    • su – postgres
    • ls 
    • vi pg_hba.conf
    • local   all             postgres                                md5
------------------------------------------------------------------------------------------------------------------------
Failed to start PostgreSQL database server.
    • Make sure tht the entries are correct in pg_hba.conf
Public repos : 
    • Ambari : http://public-repo-1.hortonworks.com/ambari/centos7/2.x/updates/2.7.3.0/
    • Hdp : http://public-repo-1.hortonworks.com/HDP/centos7/3.x/updates/3.1.0.0/
    • Hdp-utils : http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.22/repos/centos7/1.1.0.22/
    • Hdp-gpl : http://public-repo-1.hortonworks.com/HDP-GPL/centos7/3.x/updates/3.1.0.0/
------------------------------------------------------------------------------------------------------------------------

